# Apprentissage par Renforcement - Tennis Atari

## üìã Pr√©sentation du Projet

Ce projet impl√©mente et compare trois algorithmes d'apprentissage par renforcement (DQN, PPO, A2C) sur l'environnement **ALE/Tennis-v5** d'Atari.

![tennis](https://github.com/user-attachments/assets/9eb7acf7-23fe-4648-ad10-53419ccb9336)

---

## 1. L'Environnement Tennis (Atari)

### Description de l'environnement
**Tennis** est un jeu Atari classique o√π l'agent contr√¥le une raquette pour renvoyer une balle de tennis. L'objectif est de marquer des points en faisant rebondir la balle du c√¥t√© adverse sans la manquer.

### Caract√©ristiques techniques
- **Type d'observation** : Images RGB (frames du jeu)
- **Pr√©traitement** : 
  - Redimensionnement √† 84√ó84 pixels
  - Conversion en niveaux de gris
  - Stack de 4 frames cons√©cutives (pour capturer le mouvement)
  - Frame skipping (pour r√©duire la redondance temporelle)
- **Espace d'observation final** : `(4, 84, 84)` - 4 frames empil√©es de 84√ó84 pixels
- **R√©compenses** : 
  - Points positifs quand l'agent marque
  - Points n√©gatifs quand l'adversaire marque
  - L'objectif est de maximiser le score cumul√©

### Actions disponibles
L'agent peut effectuer **18 actions discr√®tes** dans l'environnement Tennis :

| Action | Description |
|--------|-------------|
| 0 | NOOP (Pas d'action) |
| 1 | FIRE (Lancer la balle) |
| 2-17 | Combinaisons de mouvements (Haut/Bas/Gauche/Droite + FIRE) |

Les actions principales sont :
- **D√©placements** : Haut, Bas, Gauche, Droite
- **FIRE** : Frapper la balle
- **Combinaisons** : Mouvements + FIRE simultan√©ment

L'agent doit apprendre √† :
1. Positionner sa raquette correctement
2. Anticiper la trajectoire de la balle
3. Frapper au bon moment
4. Renvoyer la balle vers l'adversaire

---

## 2. Algorithmes d'Apprentissage par Renforcement

Nous comparons trois algorithmes state-of-the-art pour ce probl√®me :

### DQN (Deep Q-Network)

#### Pourquoi DQN ?
DQN est un algorithme **off-policy** bas√© sur la Q-learning qui a r√©volutionn√© l'apprentissage par renforcement en 2015 en atteignant des performances humaines sur plusieurs jeux Atari. Il est particuli√®rement adapt√© pour :
- Les espaces d'actions discrets (comme Tennis avec 18 actions)
- Les observations visuelles complexes (gr√¢ce au CNN)
- L'apprentissage √† partir d'exp√©riences pass√©es

#### Param√®tres d√©taill√©s

```python
DQN(
    policy="CnnPolicy",              # Politique bas√©e sur CNN pour traiter les images
    learning_rate=1e-4,              # Taux d'apprentissage faible pour stabilit√©
    buffer_size=20_000,              # Taille du replay buffer (m√©moire d'exp√©riences)
    learning_starts=2_000,           # Commence √† apprendre apr√®s 2000 steps d'exploration
    batch_size=32,                   # Nombre d'exp√©riences par mise √† jour
    gamma=0.99,                      # Facteur d'actualisation (importance du futur)
    train_freq=4,                    # Mise √† jour tous les 4 steps
    gradient_steps=1,                # 1 √©tape de gradient par update
    target_update_interval=10_000,   # Mise √† jour du r√©seau cible tous les 10k steps
    exploration_fraction=0.20,       # 20% du temps pour diminuer l'exploration
    exploration_final_eps=0.01,      # Epsilon minimal (1% d'exploration al√©atoire)
)
```

**Explication des param√®tres cl√©s :**
- **CnnPolicy** : R√©seau de neurones convolutionnel pour traiter les images 84√ó84
- **buffer_size** : Stocke 20 000 transitions (√©tat, action, r√©compense, √©tat suivant)
- **learning_starts** : Accumule de l'exp√©rience avant d'apprendre (√©vite l'overfitting initial)
- **gamma=0.99** : Valorise fortement les r√©compenses futures (strat√©gie long-terme)
- **target_update_interval** : R√©seau cible stable pour r√©duire la variance de l'apprentissage
- **exploration** : Strat√©gie Œµ-greedy qui diminue de 1.0 √† 0.01 sur 20% de l'entra√Ænement

**Configuration environnement :**
- **1 environnement** : DQN est off-policy, il apprend depuis le replay buffer

---

### PPO (Proximal Policy Optimization)

#### Pourquoi PPO ?
PPO est un algorithme **on-policy** moderne et robuste, consid√©r√© comme l'un des meilleurs algorithmes policy gradient. Il est excellent pour :
- La stabilit√© d'apprentissage (clip des mises √† jour)
- L'efficacit√© computationnelle
- La fiabilit√© sur une grande vari√©t√© de t√¢ches

#### Param√®tres d√©taill√©s

```python
PPO(
    policy="CnnPolicy",              # Politique bas√©e sur CNN pour traiter les images
    learning_rate=2.5e-4,            # Taux d'apprentissage standard pour PPO
    n_steps=128,                     # Nombre de steps par rollout
    batch_size=128,                  # Taille des mini-batches pour l'optimisation
    n_epochs=4,                      # Nombre de passes sur les donn√©es collect√©es
    gamma=0.99,                      # Facteur d'actualisation
    gae_lambda=0.95,                 # Lambda pour Generalized Advantage Estimation
    clip_range=0.1,                  # Clip pour limiter les changements de politique
    ent_coef=0.01,                   # Coefficient d'entropie (encourage l'exploration)
    vf_coef=0.5,                     # Coefficient de la value function loss
    max_grad_norm=0.5,               # Clipping du gradient pour stabilit√©
)
```

**Explication des param√®tres cl√©s :**
- **n_steps=128** : Collecte 128 transitions avant chaque mise √† jour
- **n_epochs=4** : R√©utilise 4 fois les donn√©es collect√©es (efficacit√© d'√©chantillonnage)
- **gae_lambda=0.95** : Compromis entre biais et variance pour estimer l'advantage
- **clip_range=0.1** : Limite les changements drastiques de politique (stabilit√©)
- **ent_coef=0.01** : Bonus d'entropie pour √©viter la convergence pr√©matur√©e
- **vf_coef=0.5** : √âquilibre entre optimisation de la value function et de la policy
- **max_grad_norm=0.5** : Emp√™che les gradients explosifs

**Configuration environnement :**
- **1 environnement** : PPO fonctionne bien avec un seul environnement pour Atari

---

### A2C (Advantage Actor-Critic)

#### Pourquoi A2C ?
A2C est la version **synchrone** de A3C, un algorithme actor-critic qui combine les avantages des m√©thodes bas√©es sur la valeur et sur la politique. Il est particuli√®rement adapt√© pour :
- L'apprentissage parall√®le multi-environnements
- La convergence rapide gr√¢ce aux mises √† jour fr√©quentes
- L'efficacit√© avec plusieurs workers synchrones

#### Param√®tres d√©taill√©s

```python
A2C(
    policy="CnnPolicy",              # Politique bas√©e sur CNN pour traiter les images
    learning_rate=7e-4,              # Taux d'apprentissage √©lev√© pour A2C
    n_steps=8,                       # Nombre de steps avant mise √† jour (tr√®s fr√©quent)
    gamma=0.99,                      # Facteur d'actualisation
    gae_lambda=0.95,                 # Lambda pour GAE (r√©duit variance)
    ent_coef=0.01,                   # Coefficient d'entropie (exploration)
    vf_coef=0.25,                    # Coefficient de la value function (r√©duit vs PPO)
    max_grad_norm=0.5,               # Clipping du gradient
    rms_prop_eps=1e-5,               # Epsilon pour RMSprop (stabilit√© num√©rique)
    use_rms_prop=True,               # Utilise RMSprop au lieu d'Adam
    normalize_advantage=True,        # Normalise l'advantage (stabilit√©)
)
```

**Explication des param√®tres cl√©s :**
- **n_steps=8** : Mises √† jour tr√®s fr√©quentes (avec 4 envs = 32 transitions par update)
- **learning_rate=7e-4** : Plus √©lev√© que DQN/PPO car mises √† jour plus fr√©quentes
- **gae_lambda=0.95** : R√©duit la variance par rapport √† Œª=1.0
- **vf_coef=0.25** : Moins de poids sur la value function pour √©viter l'overfitting
- **use_rms_prop=True** : RMSprop est l'optimiseur classique pour A2C/A3C
- **normalize_advantage=True** : Normalise les advantages pour stabiliser l'apprentissage
- **rms_prop_eps=1e-5** : √âvite la division par z√©ro dans RMSprop

**Configuration environnement :**
- **4 environnements parall√®les** : A2C est con√ßu pour l'apprentissage multi-environnements synchrone
- Collecte des exp√©riences de 4 workers en parall√®le
- Am√©liore la diversit√© des donn√©es et r√©duit la corr√©lation
- Acc√©l√®re significativement la convergence

---

## 3. Configuration Exp√©rimentale

### Param√®tres communs
- **Total timesteps** : 300 000 steps d'entra√Ænement
- **Seeds** : 3 seeds diff√©rents (0, 1, 2) pour la robustesse statistique
- **√âvaluation** : 
  - DQN/PPO : Tous les 10 000 steps
  - A2C : Tous les 50 000 steps
  - 3 √©pisodes par √©valuation
  - 5 √©pisodes pour l'√©valuation finale

### Preprocessing Atari
- **AtariWrapper** : Pr√©traitement standard Atari (grayscale, resize, frame skip)
- **Frame stacking** : 4 frames cons√©cutives empil√©es
- **Frame shape** : 84√ó84 pixels

### Environnements sp√©cialis√©s
```python
# DQN : 1 environnement (off-policy avec replay buffer)
make_vec_env_dqn(seed=seed, n_stack=4)

# PPO : 1 environnement (on-policy avec rollout buffer)
make_vec_env_ppo(seed=seed, n_stack=4)

# A2C : 4 environnements parall√®les (multi-worker synchrone)
make_vec_env_a2c(seed=seed, n_envs=4, n_stack=4)
```

---

## 4. R√©sultats et Visualisation

Le notebook g√©n√®re :
1. **Courbes d'apprentissage** : Performance moyenne ¬± √©cart-type sur les 3 seeds
2. **Tableau r√©capitulatif** : Moyenne et variance des performances finales par algorithme
3. **Logs d√©taill√©s** : Temps d'entra√Ænement, √©valuations interm√©diaires

---

## 5. Utilisation

### Installation
```python
%pip install "gymnasium[atari,accept-rom-license]" stable-baselines3[extra] ale-py shimmy
```

### Entra√Ænement
Ex√©cutez les cellules du notebook dans l'ordre :
1. Installation des d√©pendances
2. Imports et configuration
3. D√©finition des environnements
4. Entra√Ænement des mod√®les
5. Visualisation des r√©sultats

### Structure du code
```
tennis (1).ipynb
‚îú‚îÄ‚îÄ Setup & imports
‚îú‚îÄ‚îÄ Configuration (ENV_ID, TIMESTEPS, SEEDS)
‚îú‚îÄ‚îÄ Fonctions de cr√©ation d'environnement (sp√©cialis√©es par algo)
‚îú‚îÄ‚îÄ Callback d'√©valuation avec barre de progression
‚îú‚îÄ‚îÄ Fonctions d'entra√Ænement (train_dqn, train_ppo, train_a2c)
‚îú‚îÄ‚îÄ Boucle d'entra√Ænement principale
‚îî‚îÄ‚îÄ Visualisation et analyse des r√©sultats
```

---

## 6. Comparaison des Algorithmes

| Algorithme | Type | Environnements | Sample Efficiency | Stabilit√© | Vitesse |
|------------|------|----------------|-------------------|-----------|---------|
| **DQN** | Off-policy | 1 | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **PPO** | On-policy | 1 | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê |
| **A2C** | On-policy | 4 | ‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |

**L√©gende :**
- **Sample Efficiency** : Capacit√© √† apprendre avec peu de donn√©es
- **Stabilit√©** : Fiabilit√© de la convergence
- **Vitesse** : Rapidit√© d'entra√Ænement (wall-clock time)

---

## R√©f√©rences

- **DQN** : Mnih et al., "Human-level control through deep reinforcement learning", Nature 2015
- **PPO** : Schulman et al., "Proximal Policy Optimization Algorithms", 2017
- **A2C/A3C** : Mnih et al., "Asynchronous Methods for Deep Reinforcement Learning", 2016
- **Stable-Baselines3** : https://stable-baselines3.readthedocs.io/

---

<<<<<<< HEAD
## üë§ Auteur
Hexa Team
Projet d'apprentissage par renforcement sur l'environnement Tennis Atari.
