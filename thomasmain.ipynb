{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a1d898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ale-py in c:\\users\\thoma\\anaconda3\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: autorom in c:\\users\\thoma\\anaconda3\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: gymnasium[atari] in c:\\users\\thoma\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (4.11.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from gymnasium[atari]) (0.0.4)\n",
      "Requirement already satisfied: click in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from autorom) (8.1.7)\n",
      "Requirement already satisfied: requests in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from autorom) (2.32.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from click->autorom) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from requests->autorom) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from requests->autorom) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from requests->autorom) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\thoma\\anaconda3\\lib\\site-packages (from requests->autorom) (2025.11.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: gymnasium 1.2.1 does not provide the extra 'accept-rom-license'\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium[atari] gymnasium[accept-rom-license] ale-py autorom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5a6667b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoROM will download the Atari 2600 ROMs.\n",
      "They will be installed to:\n",
      "\tC:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\AutoROM\\roms\n",
      "\n",
      "Existing ROMs will be overwritten.\n"
     ]
    }
   ],
   "source": [
    "!AutoROM --accept-license\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04fea17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<PassiveEnvChecker<AtariEnv<ALE/Tennis-v5>>>>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ale_py\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"ALE/Tennis-v5\")\n",
    "env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "581f13c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OrderEnforcing<PassiveEnvChecker<AtariEnv<ALE/Tennis-v5>>>>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "gym.make(\"ALE/Tennis-v5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2b0e6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import DQN, PPO, A2C\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "from stable_baselines3.common.atari_wrappers import AtariWrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85e9dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    env = gym.make(\"ALE/Tennis-v5\", frameskip=1)\n",
    "    env = AtariWrapper(env)\n",
    "    env = Monitor(env)\n",
    "    return env\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "env = VecFrameStack(env, n_stack=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc0933f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\thoma\\anaconda3\\Lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 5.65GB > 1.01GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62e+03 |\n",
      "|    ep_rew_mean      | -24      |\n",
      "|    exploration_rate | 0.795    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 1004     |\n",
      "|    time_elapsed     | 6        |\n",
      "|    total_timesteps  | 6484     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62e+03 |\n",
      "|    ep_rew_mean      | -24      |\n",
      "|    exploration_rate | 0.589    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 331      |\n",
      "|    time_elapsed     | 39       |\n",
      "|    total_timesteps  | 12986    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000176 |\n",
      "|    n_updates        | 746      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.41e+03 |\n",
      "|    ep_rew_mean      | -23.8    |\n",
      "|    exploration_rate | 0.084    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 142      |\n",
      "|    time_elapsed     | 203      |\n",
      "|    total_timesteps  | 28925    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.66e-05 |\n",
      "|    n_updates        | 4731     |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.38e+03 |\n",
      "|    ep_rew_mean      | -21.1    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 96       |\n",
      "|    time_elapsed     | 1224     |\n",
      "|    total_timesteps  | 118071   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.31e-06 |\n",
      "|    n_updates        | 27017    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 7.86e+03 |\n",
      "|    ep_rew_mean      | -21.6    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 94       |\n",
      "|    time_elapsed     | 1664     |\n",
      "|    total_timesteps  | 157194   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.14e-06 |\n",
      "|    n_updates        | 36798    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 8.68e+03 |\n",
      "|    ep_rew_mean      | -22      |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 95       |\n",
      "|    time_elapsed     | 2173     |\n",
      "|    total_timesteps  | 208244   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0155   |\n",
      "|    n_updates        | 49560    |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "dqn = DQN(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=100000,\n",
    "    learning_starts=10000,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1000,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dqn.learn(total_timesteps=300000)\n",
    "\n",
    "dqn_time = time.time() - start\n",
    "dqn_reward, _ = evaluate_policy(dqn, env, n_eval_episodes=10)\n",
    "\n",
    "dqn_reward, dqn_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d23024",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "ppo = PPO(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    learning_rate=2.5e-4,\n",
    "    n_steps=128,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "ppo.learn(total_timesteps=300000)\n",
    "\n",
    "ppo_time = time.time() - start\n",
    "ppo_reward, _ = evaluate_policy(ppo, env, n_eval_episodes=10)\n",
    "\n",
    "ppo_reward, ppo_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7a8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "a2c = A2C(\n",
    "    \"CnnPolicy\",\n",
    "    env,\n",
    "    learning_rate=7e-4,\n",
    "    gamma=0.99,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "a2c.learn(total_timesteps=300000)\n",
    "\n",
    "a2c_time = time.time() - start\n",
    "a2c_reward, _ = evaluate_policy(a2c, env, n_eval_episodes=10)\n",
    "\n",
    "a2c_reward, a2c_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc3a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [\"DQN\", \"PPO\", \"A2C\"]\n",
    "scores = [dqn_reward, ppo_reward, a2c_reward]\n",
    "times = [dqn_time, ppo_time, a2c_time]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(algos, scores)\n",
    "plt.title(\"Score moyen par algorithme â€” Tennis Atari\")\n",
    "plt.ylabel(\"Score moyen\")\n",
    "plt.show()\n",
    "\n",
    "for a, s, t in zip(algos, scores, times):\n",
    "    print(a, \"Score moyen:\", s, \"Temps:\", t)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
